<!doctype html><html lang=en-us><head><title>Beginner137</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=description content><link rel=stylesheet href=../../css/theme.min.css></head><body><div id=content class=mx-auto><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1"><div class=row><div class="col-sm-4 col-12 text-sm-right text-center pt-sm-4"><a href=../../ class=text-decoration-none><img id=home-image class=rounded-circle src=../../images/avatar.jpg></a></div><div class="col-sm-8 col-12 text-sm-left text-center"><h2 class="m-0 mb-2 mt-4"><a href=../../ class=text-decoration-none>Doris Xiang</a></h2><p class="text-muted mb-1">Builder | Learner based in Seattle</p><ul id=nav-links class="list-inline mb-2"><li class=list-inline-item><a class="badge badge-white" href=../../about/ title=About>About</a></li><li class=list-inline-item><a class="badge badge-white" href=../../posts/ title=Posts>Posts</a></li></ul><ul id=nav-social class=list-inline><li class="list-inline-item mr-3"><a href=http://github.com/beginner137 target=_blank><i class="fab fa-github fa-1x text-muted"></i></a></li><li class="list-inline-item mr-3"><a href=https://x.com/dorisxiang137 target=_blank><i class="fab fa-x fa-1x text-muted"></i></a></li></ul></div></div><hr></header><div class=container><div class="pl-sm-4 ml-sm-5"><h1 id=a-guide-to-byte-pair-encoding-from-first-principles>A Guide to Byte Pair Encoding from First Principles</h1><p>Everyone quotes token counts, but what are we actually counting?</p><p>Computers don&rsquo;t have eyes; they process integers. The challenge is bridging the gap between messy human language and clean numbers. In this post, I&rsquo;ll explain Byte Pair Encoding (BPE) from first principlesâ€”starting with why the two &ldquo;naive&rdquo; approaches might fail.</p><h4 id=how-big-should-a-token-be>How Big Should a &ldquo;Token&rdquo; Be?</h4><p>If we want to teach a computer the word &ldquo;Apple&rdquo;, we have two extremes:</p><ol><li><p><strong>Character-level:</strong> We could turn every single letter into a number (A=1, B=2,&mldr;).</p><ul><li><em>The Good:</em> We only need a tiny vocabulary for languages like English.</li><li><em>The Bad:</em> It&rsquo;s incredibly inefficient. The model has to process 5 separate tokens just to grasp the single concept of &ldquo;Apple.&rdquo; Itâ€™s like reading a book one letter at a time. -> <strong>Too slow.</strong></li></ul></li><li><p><strong>Word-level:</strong> We could assign a unique number to every single word in the English language.</p><ul><li><em>The Good:</em> It&rsquo;s fast. &ldquo;Apple&rdquo; is just one number.</li><li><em>The Bad:</em> Language is infinite. What happens when the model sees a new word like &ldquo;iPhone17&rdquo; or a typo? If it&rsquo;s not in the dictionary, the model breaks or sees an <code>&lt;Unknown></code> token. -> <strong>Too rigid.</strong></li></ul></li></ol><h4 id=the-solution-byte-pair-encoding-bpe>The Solution: Byte Pair Encoding (BPE)</h4><p>BPE is the middle ground. It keeps common words whole (like &ldquo;the&rdquo; or &ldquo;apple&rdquo;) but breaks rare or complex words into smaller, reusable chunks (like &ldquo;ing&rdquo;, &ldquo;pre&rdquo;, or &ldquo;ed&rdquo;).</p><p>This gives us the best of both worlds:</p><ol><li><p><strong>Efficiency:</strong> Common words are single tokens.</p></li><li><p><strong>Flexibility:</strong> It can build <em>any</em> wordâ€”even ones it has never seenâ€”out of sub-word pieces. &ldquo;uninstagrammable&rdquo; becomes <code>['un', 'instagram', 'mable']</code>.</p></li></ol><p>Now letâ€™s see how BPE training & inference works, step by step, with the diagram as our map.</p><p><img src=./diagram.png alt="BPE training and inference diagram"></p><br><br><h2 id=part-1-the-training-phase>Part 1: The Training Phase</h2><p>Before the AI can read, we have to build its vocabulary. This &ldquo;Training&rdquo; phase isn&rsquo;t the complex neural network training you read about in the news; it&rsquo;s effectively a statistical compression algorithm.</p><h4 id=step-1-pre-tokenization>Step 1: Pre-tokenization</h4><p>Before analyzing texts, we split them into logical units. If we blindly treated every string as unique, we would face two major headaches:</p><p><strong>1. Handling Punctuation:</strong> We don&rsquo;t want &ldquo;Hello&rdquo; and &ldquo;Hello,&rdquo; to be two different tokens. That wastes vocabulary space. So, we force a split between words and punctuation.</p><p><strong>2. Handling Spaces:</strong> If we just split by space, we lose the spaces. We wouldn&rsquo;t know if the original text was &ldquo;Hello world&rdquo; or &ldquo;Helloworld.&rdquo; To fix this, modern tokenizers <strong>glue the space to the start of the next word</strong>.</p><p>This ensures the process is <strong>reversible</strong>. We can reconstruct the sentence perfectly by joining the tokens.</p><ul><li><p><em>Raw Text:</em> &ldquo;Hello, world!&rdquo;</p></li><li><p><em>Prepped:</em> <code>["Hello", ",", " world", "!"]</code></p></li></ul><h4 id=step-2-the-byte-view>Step 2: The Byte View</h4><p>To a computer, a letter isn&rsquo;t a shape; it&rsquo;s a sequence of 1 to 4 bytes (UTF-8). This allows us to represent over <strong>1.1 million possible symbols</strong>â€”from English letters to Kanji to Emojisâ€”using a base vocabulary of just 256 bytes.</p><ul><li>h -> [104]</li><li>Space -> [32]</li><li>ðŸ¤  -> [240, 159, 164, 160] (4 bytes)</li></ul><h4 id=step-3-finding-patterns-the-merges>Step 3: Finding Patterns (The Merges)</h4><p>To understand how the merge process works, let&rsquo;s look at a toy example: <strong>&ldquo;hug pug hug pug&rdquo;.</strong> This is what it looks like before any merges:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[
</span></span><span style=display:flex><span>  [[104, 117, 103]],      # &#34;hug&#34;
</span></span><span style=display:flex><span>  [[32, 112, 117, 103]],  # &#34; pug&#34; (notice the 32 for space)
</span></span><span style=display:flex><span>  [[32, 104, 117, 103]],  # &#34; hug&#34;
</span></span><span style=display:flex><span>  [[32, 112, 117, 103]]   # &#34; pug&#34;
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>The algorithm looks at those lists of bytes and asks: <strong>&ldquo;Which two numbers appear side-by-side the most?&rdquo;</strong></p><br><h5 id=step-31-counting-pairs>Step 3.1: Counting Pairs</h5><p>We count the frequency of every neighbor pair.</p><div class=table-responsive style="margin:.5rem 0"><table style=border-collapse:collapse;width:auto><thead><tr><th style="border:1px solid #ddd;padding:6px 10px;text-align:left">Pair</th><th style="border:1px solid #ddd;padding:6px 10px;text-align:left">Corresponds to</th><th style="border:1px solid #ddd;padding:6px 10px;text-align:right">Frequency</th></tr></thead><tbody><tr><td style="border:1px solid #ddd;padding:6px 10px">(117, 103)</td><td style="border:1px solid #ddd;padding:6px 10px">u, g</td><td style="border:1px solid #ddd;padding:6px 10px;text-align:right">4</td></tr><tr><td style="border:1px solid #ddd;padding:6px 10px">(104, 117)</td><td style="border:1px solid #ddd;padding:6px 10px">h, u</td><td style="border:1px solid #ddd;padding:6px 10px;text-align:right">2</td></tr><tr><td style="border:1px solid #ddd;padding:6px 10px">â€¦</td><td style="border:1px solid #ddd;padding:6px 10px">â€¦</td><td style="border:1px solid #ddd;padding:6px 10px;text-align:right">â€¦</td></tr></tbody></table></div><br><h5 id=step-32-merging-the-winner>Step 3.2: Merging the Winner</h5><p>The pair <code>u</code> (117) and <code>g</code> (103) appears most often (4 times). We merge them into a new token, assigned the ID <strong>256</strong>.</p><ul><li><strong>Rule:</strong> (117, 103) â†’ 256</li></ul><br><h5 id=step-33-repeat>Step 3.3: Repeat</h5><p>We update our text and count again. Now we might see <code>h</code> (104) next to our new token <code>ug</code> (256). We merge those into <strong>257</strong> (&ldquo;hug&rdquo;).</p><h4 id=the-final-output>The Final Output</h4><p>At the end of training, we discard the text and keep only two things:</p><ol><li><p><strong>The Merges (Rules):</strong> An ordered list of what to combine.</p></li><li><p><strong>The Vocabulary:</strong> The dictionary mapping IDs to text.</p></li></ol><p>Example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Merges as list of lists:
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>  (117, 103), # 1st merge: u + g -&gt; ug
</span></span><span style=display:flex><span>  (104, 256), # 2nd merge: h + ug -&gt; hug
</span></span><span style=display:flex><span>  (112, 256)  # 3rd merge: p + ug -&gt; pug
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Vocabulary:
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  256: &#34;ug&#34;,
</span></span><span style=display:flex><span>  257: &#34;hug&#34;,
</span></span><span style=display:flex><span>  258: &#34;pug&#34;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>Why Merge Order Matters</strong></p><p>The order acts as a tie-breaker to prevent inconsistent splits.<br>Consider the sequence &ldquo;the&rdquo;.<br>Suppose we have two learned rules:</p><ul><li>Merge t + h -> th</li><li>Merge h + e -> he</li></ul><p>If we apply Rule 1 first, we get the tokens th, e.<br>If we apply Rule 2 first, we get the tokens t, he.</p><p>These are totally different ways to split the word! With strict order, BPE guarantees we always pick the one that was most frequent in the training data (e.g., if th appeared more often than he, we always prioritize th).</p><br><br><h2 id=part-2-the-inference-phase>Part 2: The Inference Phase</h2><p>This is what happens at runtime (e.g., when you type a prompt into ChatGPT). The model takes the &ldquo;Rules&rdquo; we learned and applies them to new text.</p><h4 id=encoding-text--numbers><strong>Encoding: Text â†’ Numbers</strong></h4><p>Let&rsquo;s say you type the word &ldquo;thug&rdquo;. The model has never seen this word before! But it knows the pieces.</p><p><strong>Prep:</strong> It breaks the word into bytes: <code>t</code>, <code>h</code>, <code>u</code>, <code>g</code>.</p><p><strong>Check Rules:</strong> It checks its rule book.</p><ul><li>&ldquo;Do I have a token for t+h?&rdquo; -> No.</li><li>&ldquo;Do I have a token for u+g?&rdquo; -> Yes! We learned ug earlier.</li></ul><p><strong>Merge:</strong> It combines them: <code>t</code>, <code>h</code>, <code>ug</code>.</p><p><strong>Final IDs:</strong> It swaps those pieces for their ID numbers: <code>[116, 104, 256]</code>.</p><p><strong>The Result:</strong> A clean, compact list of numbers that captures the meaning without needing an infinite dictionary.</p><h4 id=decoding-numbers--text><strong>Decoding: Numbers â†’ Text</strong></h4><p>When the AI responds, it just does the reverse. It spits out numbers, looks them up in the vocabulary, and glues the text back together for you to read.</p><br><h2 id=why-this-matters>Why This Matters</h2><p>BPE is the reason you can chat with AI in English, switch to Japanese, paste some Python code, and drop a ðŸ¤  emoji without the system crashing.</p><p>It gives computers the flexibility to read characters (when needed) but the efficiency to read words (most of the time).</p><br><h2 id=resources--code>Resources & Code</h2><p>This post is first of my notes series from learning <a href=https://cs336.stanford.edu/>CS336 Language Modeling from Scratch</a>.</p><p>If you want to see how this works in actual Python code, I&rsquo;ve implemented a BPE tokenizer from scratch on GitHub. I included 4 iterations of the solution, moving from a brute-force approach to a more optimized versions:</p><p>ðŸ‘‰ <a href=https://github.com/beginner137/cs336-homework-personal/tree/main/assignment1/bpe_tokenizer_training>https://github.com/beginner137/cs336-homework-personal/tree/main/assignment1/bpe_tokenizer_training</a></p><ol><li><strong>Brute Force Solution:</strong> The simplest implementation to understand the logic</li><li><strong>Optimized Solution:</strong> Improved performance for larger datasets.</li><li><strong>Multiprocessing Solution:</strong> Uses multiple CPU cores to speed up training</li><li><strong>Memory Optimized Solution:</strong> Designed to handle massive datasets without crashing RAM.</li></ol></div></div></div><footer class="text-center pb-1"><small class=text-muted>&copy; Copyright 2025, @Doris Xiang<br></small></footer></body></html>